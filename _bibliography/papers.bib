% ==========================================================================
% Maiqi Jiang â€” Publication List
% Place at: _bibliography/papers.bib
% ==========================================================================

% ------------------------------------------------------------------
% ACCEPTED / PUBLISHED
% ------------------------------------------------------------------

@inproceedings{nafee2025drike,
  title     = {Dynamic Retriever for In-Context Knowledge Editing via Policy Optimization},
  author    = {Nafee*, Mahmud Wasif and Jiang*, Maiqi and Chen, Haipeng and Zhang, Yanfu},
      editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
  booktitle = {Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  selected  = {true},
  abbr      = {EMNLP},
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.848/",
    doi = "10.18653/v1/2025.emnlp-main.848",
    pages = "16744--16757",
    ISBN = "979-8-89176-332-6",
    abstract = "Large language models (LLMs) excel at factual recall yet still propagate stale or incorrect knowledge. In{-}context knowledge editing offers a gradient-free remedy suitable for black-box APIs, but current editors rely on static demonstration sets chosen by surface-level similarity, leading to two persistent obstacles: (i) a quantity{--}quality trade-off, and (ii) lack of adaptivity to task difficulty. We address these issues by dynamically selecting supporting demonstrations according to their utility for the edit. We propose Dynamic Retriever for In-Context Knowledge Editing (DR-IKE), a lightweight framework that (1) trains a BERT retriever with REINFORCE to rank demonstrations by editing reward, and (2) employs a learnable threshold $\sigma$ to prune low-value examples, shortening the prompt when the edit is easy and expanding it when the task is hard. DR-IKE performs editing without modifying model weights, relying solely on forward passes for compatibility with black-box LLMs. On the CounterFact benchmark, it improves edit success by up to 17.1{\%}, reduces latency by 41.6{\%}, and preserves accuracy on unrelated queries{---}demonstrating scalable and adaptive knowledge editing."
}

@inproceedings{jiang2026pairs,
  title     = {PAIRS, Not Labels: Predicting Protein-Phenotype Associations via Link Prediction},
  author    = {Maiqi Jiang and Yanshuo Chen and Guodong Liu and Avinash Sahu and Ye Gao and Yanfu Zhang},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2026},
  abbr      = {ICASSP},
  selected  = {true},
  abstract  = {Accurately predicting protein-phenotype associations is essential for understanding disease mechanisms and advancing therapeutic development. While wet-lab experiments have identified numerous associations, they remain costly and labor-intensive. Computational methods offer scalable alternatives; however, many existing models oversimplify biological interactions by reducing phenotypes to class labels and neglecting the importance of phenotype semantics. To address these limitations, we propose IM-HIN, a novel framework that integrates heterogeneous information network (HIN) for protein-phenotype association prediction. By explicitly modeling proteins and phenotypes as distinct entities and formulating their relationships as a link prediction task, our approach captures complex biological interactions more effectively. Experimental evaluations on real-world datasets demonstrate the superiority of IM-HIN over existing methods, revealing novel and biologically relevant associations.}
}

@inproceedings{icassp2026eeg,
  title     = {Hierarchical Convolution Multibranch Transformer for EEG Signals},
  author    = {Omid Mersa and Maiqi Jiang and Ye Gao and Qun Li and Yanfu Zhang},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2026},
  abbr      = {ICASSP},
  selected  = {false},
  abstract  = {Electroencephalography (EEG) supports brain-computer interfaces (BCI), cognitive monitoring, and clinical diagnostics through millisecond-resolution neural recordings. However, effective representation learning remains challenging due to low signal-to-noise ratios, heterogeneous electrode configurations, and complex spatial-spectral-temporal dynamics. While deep models have improved performance, they often blur axis-specific structures, reducing generalizability across subjects and datasets. We introduce the Hierarchical Convolution Multibranch Transformer (HCMT), a self-supervised framework that maintains axis-aware representations via a spectral filter bank, hierarchical convolutions, and dedicated Transformer branches for spatial, spectral, and temporal modeling. HCMT is trained with a multi-objective loss that integrates masked reconstruction, branch decorrelation, and feature diversity. Across tasks including P300 detection, motor imagery, and sleep staging, HCMT consistently outperforms standard baselines, highlighting its effectiveness in robust, axis-aware EEG representation learning.}
}

% ------------------------------------------------------------------
% PREPRINTS
% ------------------------------------------------------------------

@article{jiang2026metaxplain,
  title   = {Is Meta-Path Attention an Explanation? Evidence of Alignment and Decoupling in Heterogeneous GNNs},
  author  = {Jiang, Maiqi and Ali, Noman and Ding, Yiran and Zhang, Yanfu},
  journal = {arXiv preprint arXiv:2602.08500},
  year    = {2026},
  arxiv   = {2602.08500},
  abbr    = {Preprint},
  selected = {true},
  abstract = {Meta-path-based heterogeneous graph neural networks aggregate over meta-path-induced views, and their semantic-level attention over meta-path channels is widely used as a narrative for ``which semantics matter.'' We study this assumption empirically by asking: when does meta-path attention reflect meta-path importance, and when can it decouple? A key challenge is that most post-hoc GNN explainers are designed for homogeneous graphs, and naive adaptations to heterogeneous neighborhoods can mix semantics and confound perturbations. To enable a controlled empirical analysis, we introduce MetaXplain, a meta-path-aware post-hoc explanation protocol that applies existing explainers in the native meta-path view domain via (i) view-factorized explanations, (ii) schema-valid channel-wise perturbations, and (iii) fusion-aware attribution, without modifying the underlying predictor. We benchmark representative gradient-, perturbation-, and Shapley-style explainers on ACM, DBLP, and IMDB with HAN and HAN-GCN, comparing against xPath and type-matched random baselines under standard faithfulness metrics. To quantify attention reliability, we propose Meta-Path Attention--Explanation Alignment (MP-AEA), which measures rank correlation between learned attention weights and explanation-derived meta-path contribution scores across random runs. Our results show that meta-path-aware explanations typically outperform random controls, while MP-AEA reveals both high-alignment and statistically significant decoupling regimes depending on the dataset and backbone; moreover, retraining on explanation-induced subgraphs often preserves, and in some noisy regimes improves, predictive performance, suggesting an explanation-as-denoising effect.
}
}

@article{li2025autognr,
  title   = {Automated Heterogeneous Network Learning with Non-Recursive Message Passing},
  author  = {Li*, Zhaoqing and Jiang*, Maiqi and Chen, Shengyuan and Li, Bo and Chen, Guorong and Huang, Xiao},
  journal = {arXiv preprint arXiv:2501.07598},
  year    = {2025},
  arxiv   = {2501.07598},
  abbr    = {Preprint},
  selected = {false},
  abstract = {Heterogeneous information networks (HINs) can be used to model various real-world systems. As HINs consist of multiple types of nodes, edges, and node features, it is nontrivial to directly apply graph neural network (GNN) techniques in heterogeneous cases. There are two remaining major challenges. First, homogeneous message passing in a recursive manner neglects the distinct types of nodes and edges in different hops, leading to unnecessary information mixing. This often results in the incorporation of ``noise'' from uncorrelated intermediate neighbors, thereby degrading performance. Second, feature learning should be handled differently for different types, which is challenging especially when the type sizes are large. To bridge this gap, we develop a novel framework - AutoGNR, to directly utilize and automatically extract effective heterogeneous information. Instead of recursive homogeneous message passing, we introduce a non-recursive message passing mechanism for GNN to mitigate noise from uncorrelated node types in HINs. Furthermore, under the non-recursive framework, we manage to efficiently perform neural architecture search for an optimal GNN structure in a differentiable way, which can automatically define the heterogeneous paths for aggregation. Our tailored search space encompasses more effective candidates while maintaining a tractable size. Experiments show that AutoGNR consistently outperforms state-of-the-art methods on both normal and large scale real-world HIN datasets.
}
}