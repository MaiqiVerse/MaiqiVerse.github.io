% ==========================================================================
% Maiqi Jiang â€” Publication List
% Place at: _bibliography/papers.bib
% ==========================================================================

% ------------------------------------------------------------------
% ACCEPTED / PUBLISHED
% ------------------------------------------------------------------

@inproceedings{nafee2025drike,
  title     = {Dynamic Retriever for In-Context Knowledge Editing via Policy Optimization},
  author    = {Nafee*, Mahmud Wasif and Jiang*, Maiqi and Chen, Haipeng and Zhang, Yanfu},
      editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
  booktitle = {Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  selected  = {true},
  abbr      = {EMNLP},
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.848/",
    doi = "10.18653/v1/2025.emnlp-main.848",
    pages = "16744--16757",
    ISBN = "979-8-89176-332-6",
    abstract = "Large language models (LLMs) excel at factual recall yet still propagate stale or incorrect knowledge. In{-}context knowledge editing offers a gradient-free remedy suitable for black-box APIs, but current editors rely on static demonstration sets chosen by surface-level similarity, leading to two persistent obstacles: (i) a quantity{--}quality trade-off, and (ii) lack of adaptivity to task difficulty. We address these issues by dynamically selecting supporting demonstrations according to their utility for the edit. We propose **D**ynamic **R**etriever for **I**n-Context **K**nowledge **E**diting (DR-IKE), a lightweight framework that (1) trains a BERT retriever with REINFORCE to rank demonstrations by editing reward, and (2) employs a *learnable threshold {\ensuremath{\sigma}}* to prune low-value examples, shortening the prompt when the edit is easy and expanding it when the task is hard. DR-IKE performs editing without modifying model weights, relying solely on forward passes for compatibility with black-box LLMs. On the CounterFact benchmark, it improves edit success by up to 17.1{\%}, reduces latency by 41.6{\%}, and preserves accuracy on unrelated queries{---}demonstrating scalable and adaptive knowledge editing."
}

@inproceedings{jiang2026pairs,
  title     = {PAIRS, Not Labels: Predicting Protein-Phenotype Associations via Link Prediction},
  author    = {Maiqi Jiang and Yanshuo Chen and Guodong Liu and Avinash Sahu and Ye Gao and Yanfu Zhang},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2026},
  abbr      = {ICASSP},
  selected  = {true},
  abstract  = {We formulate protein-phenotype association prediction as a link prediction task on biomedical graphs, bypassing the need for explicit phenotype labels and enabling scalable discovery of protein-phenotype relationships.}
}

@inproceedings{icassp2026eeg,
  title     = {Hierarchical Convolution Multibranch Transformer for EEG Signals},
  author    = {Omid Mersa and Maiqi Jiang and Ye Gao and Qun Li and Yanfu Zhang},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2026},
  abbr      = {ICASSP},
  selected  = {false},
  abstract  = {A hierarchical convolution multibranch Transformer architecture for effective EEG signal modeling, combining local convolutional feature extraction with global Transformer attention.}
}

% ------------------------------------------------------------------
% PREPRINTS
% ------------------------------------------------------------------

@article{jiang2026metaxplain,
  title   = {Is Meta-Path Attention an Explanation? Evidence of Alignment and Decoupling in Heterogeneous GNNs},
  author  = {Jiang, Maiqi and Ali, Noman and Ding, Yiran and Zhang, Yanfu},
  journal = {arXiv preprint arXiv:2602.08500},
  year    = {2026},
  arxiv   = {2602.08500},
  abbr    = {Preprint},
  selected = {true},
  abstract = {Meta-path-based heterogeneous graph neural networks aggregate over meta-path-induced views, and their semantic-level attention over meta-path channels is widely used as a narrative for ``which semantics matter.'' We study this assumption empirically by asking: when does meta-path attention reflect meta-path importance, and when can it decouple? A key challenge is that most post-hoc GNN explainers are designed for homogeneous graphs, and naive adaptations to heterogeneous neighborhoods can mix semantics and confound perturbations. To enable a controlled empirical analysis, we introduce MetaXplain, a meta-path-aware post-hoc explanation protocol that applies existing explainers in the native meta-path view domain via (i) view-factorized explanations, (ii) schema-valid channel-wise perturbations, and (iii) fusion-aware attribution, without modifying the underlying predictor. We benchmark representative gradient-, perturbation-, and Shapley-style explainers on ACM, DBLP, and IMDB with HAN and HAN-GCN, comparing against xPath and type-matched random baselines under standard faithfulness metrics. To quantify attention reliability, we propose Meta-Path Attention--Explanation Alignment (MP-AEA), which measures rank correlation between learned attention weights and explanation-derived meta-path contribution scores across random runs. Our results show that meta-path-aware explanations typically outperform random controls, while MP-AEA reveals both high-alignment and statistically significant decoupling regimes depending on the dataset and backbone; moreover, retraining on explanation-induced subgraphs often preserves, and in some noisy regimes improves, predictive performance, suggesting an explanation-as-denoising effect.
}
}

@article{li2025autognr,
  title   = {Automated Heterogeneous Network Learning with Non-Recursive Message Passing},
  author  = {Li*, Zhaoqing and Jiang*, Maiqi and Chen, Shengyuan and Li, Bo and Chen, Guorong and Huang, Xiao},
  journal = {arXiv preprint arXiv:2501.07598},
  year    = {2025},
  arxiv   = {2501.07598},
  abbr    = {Preprint},
  selected = {false},
  abstract = {Heterogeneous information networks (HINs) can be used to model various real-world systems. As HINs consist of multiple types of nodes, edges, and node features, it is nontrivial to directly apply graph neural network (GNN) techniques in heterogeneous cases. There are two remaining major challenges. First, homogeneous message passing in a recursive manner neglects the distinct types of nodes and edges in different hops, leading to unnecessary information mixing. This often results in the incorporation of ``noise'' from uncorrelated intermediate neighbors, thereby degrading performance. Second, feature learning should be handled differently for different types, which is challenging especially when the type sizes are large. To bridge this gap, we develop a novel framework - AutoGNR, to directly utilize and automatically extract effective heterogeneous information. Instead of recursive homogeneous message passing, we introduce a non-recursive message passing mechanism for GNN to mitigate noise from uncorrelated node types in HINs. Furthermore, under the non-recursive framework, we manage to efficiently perform neural architecture search for an optimal GNN structure in a differentiable way, which can automatically define the heterogeneous paths for aggregation. Our tailored search space encompasses more effective candidates while maintaining a tractable size. Experiments show that AutoGNR consistently outperforms state-of-the-art methods on both normal and large scale real-world HIN datasets.
}
}